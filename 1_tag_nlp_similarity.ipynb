{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\vir_py3.7\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/Wemap_Category_url.json\", encoding=\"utf-8\", mode=\"r\") as f:\n",
    "    categorys = list(json.load(f).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/eng/스콥카테고리.json\", encoding=\"utf-8\", mode=\"r\") as f:\n",
    "    json_file = json.load(f)\n",
    "    big_category = json_file.keys()\n",
    "    categorys = list(big_category) \\\n",
    "                + [cur for key in big_category for cur in json_file[key]]\\\n",
    "                + [key + \" \" + cur for key in big_category for cur in json_file[key]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위메프 regex\n",
    "import re\n",
    "def apply_regex_wemap(text):\n",
    "    category_list = text.split(\"<\")\n",
    "    \n",
    "    out_category = []\n",
    "    for cur in category_list:\n",
    "        out_category.append(re.sub('[<>/]', \" \", cur))\n",
    "    okt_noun_list = okt.nouns(\" \".join(out_category[1:]))\n",
    "    return \" \".join(okt_noun_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스콥 regex\n",
    "import re\n",
    "def apply_regex_scop(text):\n",
    "    # category_list = text.split(\" \")\n",
    "\n",
    "    # out_category = []\n",
    "    # for cur in category_list:\n",
    "    #     out_category.append(re.sub('[<>/]', \" \", cur))\n",
    "\n",
    "    # okt_noun_list = okt.nouns(\" \".join(out_category))\n",
    "    return text #\" \".join(okt_noun_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_category = [apply_regex_wemap(cur) for cur in categorys]\n",
    "total_category = [apply_regex_scop(cur) for cur in categorys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/eng/nodes.txt\", encoding=\"utf-8\", mode=\"r\") as f:\n",
    "    tags = [re.sub(\"\\n\",\"\",cur) for cur in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at snunlp/KR-BERT-char16424 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# 한글\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"snunlp/KR-BERT-char16424\")\n",
    "# model = BertModel.from_pretrained(\"snunlp/KR-BERT-char16424\")\n",
    "\n",
    "# 영어\n",
    "tokenizer = BertTokenizer.from_pretrained(\"snunlp/KR-BERT-char16424\")\n",
    "model = BertModel.from_pretrained(\"snunlp/KR-BERT-char16424\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF 테이블 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token(sentence):\n",
    "    tokens = tokenizer.encode(sentence, add_special_tokens=True, return_tensors=\"pt\").tolist()[0]\n",
    "    return \" \".join([str(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = [get_token(category) for category in total_category+tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "\n",
    "tfidf_vectors = vectorizer.fit_transform(token_list)\n",
    "tfidf_words = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF 기반 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프로그레스 바를 출력하는 함수\n",
    "def print_progress_bar(iteration, total, prefix='Complete', suffix='', decimals=5, length=100, fill='█', print_end='\\r'):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        iteration (int): 현재 진행된 작업 수\n",
    "        total (int): 전체 작업 수\n",
    "        prefix (str): 프로그레스 바 앞에 추가할 텍스트\n",
    "        suffix (str): 프로그레스 바 뒤에 추가할 텍스트\n",
    "        decimals (int): 백분율에서 소수점 이하 자릿수\n",
    "        length (int): 프로그레스 바의 총 길이 (글자 수)\n",
    "        fill (str): 프로그레스 바의 채우는 문자\n",
    "        print_end (str): 프로그레스 바 출력 후 커서의 위치를 조정하는 문자열\n",
    "    \"\"\"\n",
    "    # 현재 진행된 작업의 백분율 계산\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "\n",
    "    # 프로그레스 바의 채워진 길이 계산\n",
    "    filled_length = int(length * iteration // total)\n",
    "\n",
    "    # 프로그레스 바 출력\n",
    "    bar = fill * filled_length + '-' * (length - filled_length)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end=print_end)\n",
    "\n",
    "    # 모든 작업이 끝나면 줄바꿈\n",
    "    if iteration == total:\n",
    "        print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TFIDF O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targettokens_and_TFIDFvalues(tokens, st_idx):\n",
    "    token_idx = tfidf_vectors[st_idx].indices\n",
    "    token_id_pairdict = dict(zip(tfidf_words[token_idx], token_idx))\n",
    "\n",
    "    target_tokens = []\n",
    "    for token in tokens[0].tolist():\n",
    "        if str(token) in list(token_id_pairdict.keys()):\n",
    "            target_tokens.append(token)\n",
    "    \n",
    "    tfidf_value_list =[tfidf_vectors[st_idx, token_id_pairdict[str(token)]] for token in target_tokens]\n",
    "\n",
    "    return torch.IntTensor(target_tokens), torch.FloatTensor(tfidf_value_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector_tfidf(sentence, st_idx):\n",
    "    tokens = tokenizer.encode(sentence, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    target_tokens, tfidf_value_list = get_targettokens_and_TFIDFvalues(tokens, st_idx)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output = model(target_tokens.reshape(1,-1))\n",
    "        last_hidden_states = model_output[0]\n",
    "\n",
    "    st_tfidf_vec = last_hidden_states * tfidf_value_list.reshape(-1,1)\n",
    "    sentence_vector = torch.mean(st_tfidf_vec, dim=1)\n",
    "\n",
    "    return sentence_vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_matrix_tfidf(sentence_list, st_idx_plus = 0):\n",
    "    \n",
    "    total = len(sentence_list)\n",
    "    iteration = 0\n",
    "    \n",
    "    sentence_vector_list = []\n",
    "    \n",
    "    for idx, sentence in enumerate(sentence_list):\n",
    "        try:\n",
    "            sentence_vector_list.append(get_sentence_vector_tfidf(sentence, idx + st_idx_plus))\n",
    "\n",
    "            iteration += 1\n",
    "            print_progress_bar(iteration, total)\n",
    "        except:\n",
    "            sentence_vector_list.append(torch.zeros(len(sentence_vector_list[0])))\n",
    "            print(\"error\")\n",
    "    \n",
    "    return sentence_vector_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TFIDF X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(sentence):\n",
    "    tokens = tokenizer.encode(sentence, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output = model(tokens.reshape(1,-1))\n",
    "        last_hidden_states = model_output[0]\n",
    "    sentence_vector = torch.mean(last_hidden_states, dim=1)\n",
    "    return sentence_vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_matrix(sentence_list):\n",
    "    \n",
    "    total = len(sentence_list)\n",
    "    iteration = 0\n",
    "    \n",
    "    sentence_vector_list = []\n",
    "    for idx, sentence in enumerate(sentence_list):\n",
    "\n",
    "        sentence_vector_list.append(get_sentence_vector(sentence))\n",
    "\n",
    "        iteration += 1\n",
    "        print_progress_bar(iteration, total)\n",
    "    \n",
    "    return sentence_vector_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문장벡터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00000% \n"
     ]
    }
   ],
   "source": [
    "category_st_vec_list = get_sentence_matrix_tfidf(total_category)\n",
    "# category_st_vec_list = get_sentence_matrix(total_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete |███████████████████████████-------------------------------------------------------------------------| 27.27919% \r"
     ]
    }
   ],
   "source": [
    "tag_stvec_list = get_sentence_matrix_tfidf(tags, len(categorys))\n",
    "# tag_stvec_list = get_sentence_matrix(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_cosine_sim(vector1, vector2):\n",
    "    similarity = F.cosine_similarity(vector1, torch.stack(vector2, dim=0))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00000% \n"
     ]
    }
   ],
   "source": [
    "category_and_tag_similarity = []\n",
    "\n",
    "total = len(category_st_vec_list)\n",
    "iteration = 0\n",
    "\n",
    "for category_vec in category_st_vec_list:\n",
    "    cos_sim = cal_cosine_sim(category_vec, tag_stvec_list)\n",
    "    category_and_tag_similarity.append(cos_sim.tolist())\n",
    "\n",
    "    iteration += 1\n",
    "    print_progress_bar(iteration, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_tag_sim_dict = {}\n",
    "\n",
    "for idx, category in enumerate(total_category):\n",
    "\n",
    "    tag_sim_pair = list(zip(tags, category_and_tag_similarity[idx]))\n",
    "    sorted_tag_sim_pair = sorted(tag_sim_pair, key = lambda x:x[1], reverse= True)\n",
    "    category_tag_sim_dict[category] = sorted_tag_sim_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/category_tag_sim(TFIDF).json\", encoding=\"utf-8\", mode=\"w\") as f:\n",
    "    json.dump(category_tag_sim_dict, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['브랜드 여성 의류 원피스', '브랜드 여성 의류 티셔츠', '브랜드 여성 의류 블라우스 셔츠', '브랜드 여성 의류 투맨 후드', '브랜드 여성 의류 니트 가디건 조끼', '브랜드 여성 의류 자켓 코트', '브랜드 여성 의류 패딩 점퍼 상', '브랜드 여성 의류 가죽 모피 무스 탕', '브랜드 여성 의류 정장 세트', '브랜드 여성 의류 스커트 치마', '브랜드 여성 의류 캐 주얼 바지 팬츠', '브랜드 여성 의류 청바지', '브랜드 여성 의류 트레이닝복', '브랜드 여성 의류 수영복 비치 웨어', '브랜드 남성 의류 티셔츠', '브랜드 남성 의류 셔츠 남방', '브랜드 남성 의류 투맨 후드', '브랜드 남성 의류 니트 가디건 조끼', '브랜드 남성 의류 자켓 코트', '브랜드 남성 의류 패딩 점퍼 상', '브랜드 남성 의류 정장', '브랜드 남성 의류 캐 주얼 바지 팬츠', '브랜드 남성 의류 청바지', '브랜드 남성 의류 수영복 비치 웨어', '브랜드 진 캐 주얼 티셔츠 셔츠', '브랜드 진 캐 주얼 원피스 스커트', '브랜드 진 캐 주얼 투맨 후드', '브랜드 진 캐 주얼 니트 가디건 조끼', '브랜드 진 캐 주얼 자켓 코트', '브랜드 진 캐 주얼 패딩 점퍼 상', '브랜드 진 캐 주얼 팬츠', '브랜드 언더웨어 잠옷 홈웨어 웨어', '브랜드 언더웨어 남성 팬티', '브랜드 언더웨어 남성 내의', '브랜드 언더웨어 브라 탑 런닝 바지', '브랜드 언더웨어 여성 팬티', '브랜드 언더웨어 여성 내의', '브랜드 언더웨어 남성 런닝', '브랜드 언더웨어 보정속옷', '브랜드 언더웨어 여성 브라', '브랜드 언더웨어 여성 브라 팬티 세트', '브랜드 언더웨어 테마 속옷', '브랜드 언더웨어 속옷 악세사리', '브랜드 가방 잡화 가방', '브랜드 가방 잡화 지갑 벨트', '브랜드 가방 잡화 패션 잡화 소품', '브랜드 가방 잡화 선글라스 안경 테', '브랜드 신발 여성화', '브랜드 신발 남성화', '브랜드 신발 캐 주얼', '브랜드 신발 신발 용품', '브랜드 쥬얼리 시계 귀걸이 피어싱', '브랜드 쥬얼리 시계 목걸이 브로치', '브랜드 쥬얼리 시계 팔찌 발찌', '브랜드 쥬얼리 시계 반지 커플링', '브랜드 쥬얼리 시계 쥬얼리', '브랜드 쥬얼리 시계 시계', '브랜드 쥬얼리 시계 쥬얼리 세트 용품', '브랜드 쥬얼리 시계 골드바 돌 반지 금제품', '브랜드 쥬얼리 시계 헤어 액세서리', '브랜드 쥬얼리 시계 아동 용 쥬얼리', '수입 명품 명품 여성 가방', '수입 명품 명품 남성 가방', '수입 명품 명품 지갑 벨트', '수입 명품 명품 시계', '수입 명품 명품 슈즈', '수입 명품 명품 의류', '수입 명품 명품 잡화', '수입 명품 리 퍼브 중고 명품', '브랜드 스포츠 패션 스포츠 신발 운동화', '브랜드 스포츠 패션 여성 스포츠 의류', '브랜드 스포츠 패션 스포츠 가방 모자', '브랜드 스포츠 패션 남성 스포츠 의류', '브랜드 스포츠 패션 스포츠 양말 장갑', '브랜드 스포츠 패션 스포츠 웨어', '브랜드 스포츠 패션 스포츠 잡화', '브랜드 아웃도어 등산 아웃도어 잡화', '브랜드 아웃도어 등산 아웃도어 의류', '브랜드 아웃도어 등산 아웃도어 용품', '브랜드 아웃도어 등산 아웃도어 신발', '여성 의류 원피스', '여성 의류 티셔츠 투맨 후드', '여성 의류 블라우스 셔츠', '여성 의류 청바지', '여성 의류 바지 레깅스', '여성 의류 스커트 치마', '여성 의류 트레이닝복', '여성 의류 니트 가디건 조끼', '여성 의류 자켓 코트', '여성 의류 패딩 점퍼 상', '여성 의류 정장 세트', '여성 의류 수영복 비치 웨어', '여성 의류 마담 의류', '여성 의류 테마 의류', '남성 의류 티셔츠', '남성 의류 투맨 후드', '남성 의류 청바지', '남성 의류 셔츠 남방', '남성 의류 자켓 코트', '남성 의류 캐 주얼 바지 팬츠', '남성 의류 니트 가디건 조끼', '남성 의류 패딩 점퍼 상', '남성 의류 정장', '남성 의류 스포츠 아웃도어', '남성 의류 빅사 이즈', '남성 의류 수영복 비치 웨어', '남성 의류 테마 의류', '언더웨어 남성 팬티', '언더웨어 여성 팬티', '언더웨어 잠옷 홈웨어 웨어', '언더웨어 남성 내의', '언더웨어 남성 런닝', '언더웨어 보정속옷', '언더웨어 브라 탑 런닝 바지', '언더웨어 여성 내의', '언더웨어 여성 브라', '언더웨어 여성 브라 팬티 세트', '언더웨어 테마 속옷', '언더웨어 속옷 악세사리', '가방 잡화 가방', '가방 잡화 양말 스타킹', '가방 잡화 패션 잡화 소품', '가방 잡화 지갑 벨트', '가방 잡화 선글라스 안경 테', '가방 잡화 스포츠 아웃도어 잡화', '신발 여성화', '신발 남성화', '신발 캐 주얼', '신발 스포츠 신발 운동화', '신발 신발 용품', '쥬얼리 시계 쥬얼리', '쥬얼리 시계 귀걸이 피어싱', '쥬얼리 시계 목걸이 브로치', '쥬얼리 시계 팔찌 발찌', '쥬얼리 시계 반지 커플링', '쥬얼리 시계 쥬얼리 세트 용품', '쥬얼리 시계 시계', '쥬얼리 시계 골드바 돌 반지 금제품', '쥬얼리 시계 헤어 액세서리', '쥬얼리 시계 아동 용 쥬얼리', '패션 로드 여성 상의', '패션 로드 여성 원피스', '패션 로드 여성 하의 세트', '패션 로드 여성 신발', '패션 로드 여성 가방', '패션 로드 여성 주얼리', '패션 로드 여성 패션 잡화', '패션 로드 여성 시즌 웨어 언더웨어', '패션 로드 남성 상의', '패션 로드 남성 하의 세트', '패션 로드 남성 신발', '패션 로드 남성 가방', '패션 로드 남성 패션 잡화', '패션 로드 남성 시즌 웨어 언더웨어', '패션 쇼핑몰 원피스', '패션 쇼핑몰 티셔츠 투맨 후드', '패션 쇼핑몰 블라우스 셔츠', '패션 쇼핑몰 청바지 팬츠 레깅스', '패션 쇼핑몰 스커트 치마', '패션 쇼핑몰 트레이닝복', '패션 쇼핑몰 니트 가디건 조끼', '패션 쇼핑몰 자켓 코트', '패션 쇼핑몰 패딩 점퍼 상', '패션 쇼핑몰 정장 세트', '패션 쇼핑몰 빅사 이즈', '패션 쇼핑몰 수영복 비치 웨어', '패션 쇼핑몰 신발 가방 잡화', '패션 쇼핑몰 테마 의류', '스킨 케어 스킨 토너', '스킨 케어 로션 에멀젼', '스킨 케어 앰플 에센스 럼', '스킨 케어 오일', '스킨 케어 크림', '스킨 케어 미스트', '스킨 케어 팩 마스크', '스킨 케어 스킨 케어 세트', '메이크업 베이스 메이크업', '메이크업 아이 메이크업', '메이크업 립 메이크업', '메이크업 치 하이라이트 쉐이딩', '선 케어 선크림 선 블록', '선 케어 선밤 선 스틱', '선 케어 선 파우더 쿠션', '선 케어 선 스프레이', '클렌징 필링 클렌징', '클렌징 필링 필링 젤 스크럽', '남성 화장품 남성 스킨 케어', '남성 화장품 남성 메이크업', '향수 여성 향수', '향수 남성 향수', '향수 섬유 향수', '명품 화장품 스킨 케어', '명품 화장품 메이크업', '명품 화장품 남성 화장품', '네일케어 팁 스티커', '네일케어 매니큐어', '네일케어 젤 네일', '네일케어 큐티쿨 영양', '네일케어 리무 버', '네일케어 네일케어 도구 소품', '네일케어 네일 세트', '미용 소품 기기 화장 솜 면봉', '미용 소품 기기 아이 소품', '미용 소품 기기 페이스 소품', '미용 소품 기기 클렌 징 소품', '미용 소품 기기 헤어 소품', '미용 소품 기기 미용 기기', '미용 소품 기기 미용 소품 기타', '미용 소품 기기 타투 스티커', '바디 헤어 바디 슬 리밍', '바디 헤어 바디 로션 핸드 풋', '바디 헤어 바디 케어 워시 제모', '바디 헤어 샴푸 린스 헤어 케어', '바디 헤어 헤어 염색 파마 왁스', '기저귀 분유 유아식 기저귀', '기저귀 분유 유아식 분유', '기저귀 분유 유아식 유아식', '육아용품 스킨 케어', '육아용품 발육 용품', '육아용품 생활용품', '육아용품 수유 용품', '육아용품 임산부 출산 용품', '유 아동 의류 트렌드 유아 의류', '유 아동 의류 브랜드 유아 의류', '유 아동 신발 잡화 트렌드 아동 신발 잡화', '유 아동 신발 잡화 브랜드 아동 신발 잡화', '유 아동 청소년 도서 중고 유 아동 학습 책', '장난감 캐릭터 완구', '장난감 작동 완구', '장난감 인형 역할', '장난감 교육 완구', '장난감 신생아 완구', '장난감 스포츠 시즌 완구', '장난감 블록 게임 완구', '신선 식품 쌀 잡곡', '신선 식품 과일', '신선 식품 견과', '신선 식품 채소', '신선 식품 정육 계란', '신선 식품 가공 육 양념 육류', '신선 식품 수산', '신선 식품 김치 반찬', '신선 식품 선물 세트', '신선 식품 지역 명물', '가공 식품 과자 베이커리 떡', '가공 식품 라면 면류', '가공 식품 조리 식품', '가공 식품 조미료 소스 설탕', '가공 식품 통조림 식용유 잼', '가공 식품 치즈 버터 유 가공', '가공 식품 쿠킹 박스', '가공 식품 선물 세트', '건강 식품 다이어트 양제 비타민 건강 환', '건강 식품 다이어트 다이어트 보조 식품', '건강 식품 다이어트 홍삼 인삼', '건강 식품 다이어트 건강 즙 자양 숙취', '건강 식품 다이어트 선물 세트', '건강 식품 다이어트 지역 명물', '커피 음료 커피 코코아', '커피 음료 생수 음료', '커피 음료 우유 두유', '커피 음료 차 티백 분말', '커피 음료 전통주', '커피 음료 선물 세트', '화장지 물티슈 생리대 화장지 물티슈', '화장지 물티슈 생리대 생리대 성인 기저귀', '세제 구강 세탁 세제 섬유 유연제', '세제 구강 주방 세제 정제', '세제 구강 습 방향 살충제', '세제 구강 칫솔 면도 비누', '세제 구강 생활용품', '생활용품 가정 생활 잡화', '생활용품 디지털 도어 락 보안 용품', '생활용품 수납 정리 용품', '생활용품 욕실 목욕 용품', '생활용품 세탁 용품', '생활용품 청소 용품', '주방용품 보관 밀폐 용기', '주방용품 보온 보냉 용품', '주방용품 와인 주류 용품', '주방용품 조리 도구', '주방용품 프라이팬 냄비', '주방용품 그릇 홈 세트', '주방용품 용품', '주방용품 베이 킹 용품', '주방용품 커피 용품', '주방용품 주방 수납', '주방용품 주방 잡화', '주방용품 물병 물통', '주방용품 양념 통 일병', '주방용품 수저 커트 러리', '주방용품 쌀통 잡곡 통', '건강 의료 용품 마스크 방한', '건강 의료 용품 건강 관리 용품', '건강 의료 용품 건강 측정 용품', '건강 의료 용품 실버 의료 용품', '건강 의료 용품 안마 용품', '건강 의료 용품 건강 가전', '정수기 렌탈 월 렌탈료', '공기청정기 렌탈 월 렌탈료', '주방 가전 렌탈 전기레인지 인덕션 렌탈', '주방 가전 렌탈 식기세척기 렌탈', '주방 가전 렌탈 음식물 처리기 렌탈', '주방 가전 렌탈 기타 주방 가전 렌탈', '생활 가전 렌탈 비데 렌탈', '생활 가전 렌탈 안마 의자 렌탈', '생활 가전 렌탈 건조기 의류 관리 기 렌탈', '생활 가전 렌탈 냉장고 김치냉장고 렌탈', '생활 가전 렌탈 에어컨 렌탈', '생활 가전 렌탈 생활 가전 기타', '건강 미용 서비스 렌탈 건강 미용 용품 렌탈', '건강 미용 서비스 렌탈 유아 용품 렌탈', '건강 미용 서비스 렌탈 반려동물 용품 렌탈', '건강 미용 서비스 렌탈 가구 렌탈', '건강 미용 서비스 렌탈 자동차 타이어 렌탈', '건강 미용 서비스 렌탈 정기 구독 서비스 기타', '가구 시공 어린이 서재 가구', '가구 시공 침실 가구', '가구 시공 거실 가구', '가구 시공 주방 가구', '가구 시공 시공 리 모델링', '가구 시공 야외 이외 가구', '가구 시공 유아 어린이 가구', '가구 시공 학생 무용 가구', '침구 커튼 침구', '침구 커튼 커튼 블라인드', '침구 커튼 카페트 러그', '침구 커튼 홈 패션 용품', '조명 인테리어 조명 스탠드', '조명 인테리어 캔들 디퓨져', '조명 인테리어 홈리뉴얼', '조명 인테리어 인테리어 소품', '강아지 사료 건 사료', '강아지 사료 습 및 기타 사료', '강아지 사료 소 중형 견 사료', '강아지 사료 대형 노령 견 사료', '강아지 간식 껌', '강아지 간식 육포 키 트릿', '강아지 간식 습 파우치 캔', '강아지 간식 수제 유기농 간식', '강아지 간식 비스켓 케이크', '강아지 간식 기타 간식', '강아지 용품 배변 용품', '강아지 용품 미용 목욕 용품', '강아지 용품 의류 패션 잡화', '강아지 용품 하우스 실내 용품', '강아지 용품 외출 용품', '강아지 용품 장난감 훈련 용품', '강아지 용품 양제 위생 건강 관리', '고양이 사료 건 사료', '고양이 사료 습 및 기타 사료', '고양이 간식 습 파우치 캔', '고양이 간식 육포 키 트릿', '고양이 간식 훈제고기 소시지', '고양이 간식 수제 캣닢 기타', '고양이 용품 배변 용품', '고양이 용품 미용 목욕 용품', '고양이 용품 실내 용품', '고양이 용품 장난감', '고양이 용품 양제 위생 건강 관리', '관상어 용품 어항 수조', '관상어 용품 수조 청소 용품', '관상어 용품 수조 부속품 장식품', '관상어 용품 관상어 거북이 사료', '동물 가축 용품 동물 용품', '동물 가축 용품 새 조류 용품', '동물 가축 용품 곤충 애완동물 용품', '동물 가축 용품 이외 동물 용품', '대형 가전', '대형 가전 영상 가전', '대형 가전 냉장고', '대형 가전 세탁기', '대형 가전 의류건조기 관리 기', '대형 가전 김치냉장고', '대형 가전 중고 리퍼 반품 전시', '노트북', '노트북 삼성', '노트북 한성 컴퓨터', '노트북 주연테크', '노트북 이외 브랜드', '노트북 중고 리퍼', '데스크탑', '데스크탑 삼성', '데스크탑 주연테크', '데스크탑 한성', '데스크탑 이외 브랜드', '데스크탑 미니', '데스크탑 스틱', '데스크탑 워크스테이션', '계절 가전 에어컨 청정 가전', '계절 가전 에어컨 전기 매트 요', '계절 가전 에어컨 난방 가전', '계절 가전 에어컨 보일러 수기', '계절 가전 에어컨 선풍기 써큘 레이 터', '계절 가전 에어컨 에어컨', '계절 가전 에어컨 여름 가전', '디지털 휴대폰 카메라 휴대폰 카메라 저장장치', '디지털 휴대폰 카메라 카메라 렌즈', '디지털 휴대폰 카메라 학습 기기', '디지털 휴대폰 카메라 휴대폰', '디지털 휴대폰 카메라 휴대폰 액세서리', '디지털 휴대폰 카메라 카메라', '디지털 휴대폰 카메라 카메라 액세서리', '디지털 휴대폰 카메라 동영상', '주방 가전 전기밥솥', '주방 가전 전자레인지', '주방 가전 에어 프라이어 전기 오븐', '주방 가전 전기레인지 인덕션', '주방 가전 전기포트 주전자', '주방 가전 식기 세척 건조기', '주방 가전 믹서기 핸드 블 랜더', '주방 가전 가스레인지 렌 후드', '주방 가전 전기 그릴 팬', '주방 가전 음식물 처리기', '주방 가전 커피메이커', '주방 가전 홈 메이드 가전', '생활 가전 청소기', '생활 가전 청소기 용품', '생활 가전 다리미', '생활 가전 보풀 거기 재봉틀', '생활 가전 살균 소독 기기', '생활 가전 이외 생활 가전', '미용 가전 전기 면도기', '미용 가전 헤어 스타', '미용 가전 모기', '미용 가전 피부관리 기기', '미용 가전 전동칫솔 칫솔 모', '미용 가전 구강 청정기', '미용 가전 헤어 드라이기', '태블릿 태블릿', '태블릿 태블릿 액세서리', '모니터 프린터 모니터', '모니터 프린터 모니터 주변기기', '모니터 프린터 프린터', '모니터 프린터 복합기', '모니터 프린터 스캐너', '모니터 프린터 팩스', '모니터 프린터 잉크 토너', '모니터 프린터 프린터 용품', '부품 주변기기 저장 공유기 네트워크', '부품 주변기기 저장 저장장치', '부품 주변기기 저장 주변기기', '부품 주변기기 저장 부품', '부품 주변기기 저장 노트북 액세서리', '게임기 게임 타이틀 게임기 본체', '게임기 게임 타이틀 게임 타이틀', '게임기 게임 타이틀 게임 주변기기', '음향 기기 이어폰 헤드폰', '음향 기기 스피커', '음향 기기 음향 가전', '유아 도서 유아', '유아 도서 어린이', '유아 도서 청소년', '국내 도서 소설 시 에세이', '국내 도서 경제 경영', '국내 도서 자기계발', '국내 도서 가정 살림', '국내 도서 건강 취미', '국내 도서 여행', '국내 도서 인문 사회과학', '국내 도서 만화 라이트노벨', '국내 도서 역사 자연과학', '국내 도서 종교 역학', '국내 도서 예술 대중문화', '국내 도서 잡지', '국내 도서 중고 도서', '국내 도서 중고 음반', '국내 도서 추천 도서', '학습 이러닝 온라인교육', '학습 이러닝 학습 참고서', '학습 이러닝 대학 교재', '학습 이러닝 수험 자격증', '음반 굿 블루레이', '음반 굿 음반', '외국 도서 해외 원서', '문구 팬 용품 노트 수첩 다이어리', '문구 팬 용품 팬시 포장 용품', '문구 팬 용품 명함 상장 트로피', '문구 팬 용품 스탬프 도장', '문구 팬 용품 앨범 스크랩 북', '문구 팬 용품 편지지 엽서 봉투', '문구 팬 용품 필기 도구', '문구 팬 용품 학습 준비물', '사무 오피스 용품 데스크 정리 용품', '사무 오피스 용품 사무 비품', '사무 오피스 용품 사무 포장 접착 용품', '사무 오피스 용품 화일 바인더', '사무 오피스 용품 보드 칠판 광고', '사무기기 사무 용지 복사 용지', '사무기기 사무 용지 사무기기', '사무기기 사무 용지 전용', '미술 화방 용품 스케치 드로잉 용품', '미술 화방 용품 동양화 용품', '미술 화방 용품 물감 채색 용품', '미술 화방 용품 붓', '미술 화방 용품 미술 보조 용품', '미술 화방 용품 스케치북 미술 지류', '미술 화방 용품 어린이 미술 용품', '미술 화방 용품 제도 용품', '미술 화방 용품 조소 판화 용품', '취미 수집 파티 용품 교육 놀이 인형', '취미 수집 파티 용품 취미', '취미 수집 파티 용품 종교 용품', '취미 수집 파티 용품 보드게임 민속놀이', '취미 수집 파티 용품 퍼즐 블록', '취미 수집 파티 용품 수집 마술 용품', '취미 수집 파티 용품 이벤트 파티 용품', '꽃 원예 원예 용품 꽃 식물 화분', '꽃 원예 원예 용품 꽃 배달 당일 배송', '꽃 원예 원예 용품 선물 용꽃', '꽃 원예 원예 용품 씨앗 모종 묘목', '꽃 원예 원예 용품 원예 용품', '꽃 원예 원예 용품 조화 가든 용품', '꽃 원예 원예 용품 크리스마스 트리 소품', '악기 악기 용품 피아노 건반악기', '악기 악기 용품 관악기', '악기 악기 용품 현악기', '악기 악기 용품 교재 용 악기', '악기 악기 용품 국악기', '악기 악기 용품 기타', '악기 악기 용품 드럼 타악기', '악기 악기 용품 악기 소품', '자동차 용품 내부 용품', '자동차 용품 외부 용품', '자동차 용품 오토바이 용품', '자동차 용품 자동차 기기', '자동차 용품 타이어 휠', '자동차 용품 엔진오일 배터리', '자동차 용품 부품 튜닝', '공구 설비 자재 공구', '공구 설비 자재 안전 용품', '공구 설비 자재 전기 산업 자재', '스포츠 의류 운동화 남성 스포츠 의류', '스포츠 의류 운동화 여성 스포츠 의류', '스포츠 의류 운동화 스포츠 신발 운동화', '스포츠 의류 운동화 스포츠 가방 모자', '스포츠 의류 운동화 스포츠 웨어', '스포츠 의류 운동화 스포츠 양말 장갑', '스포츠 의류 운동화 스포츠 잡화', '휘트니 스 요가 헬스', '휘트니 스 요가 요가 필라테스', '수영 수상 레저 수영복 쉬 가드', '수영 수상 레저 수영 용품', '수영 수상 레저 이외 수상 스포츠', '등산 아웃도어 등산 아웃도어 의류', '등산 아웃도어 등산 아웃도어 신발', '등산 아웃도어 등산 아웃도어 용품', '등산 아웃도어 등산 아웃도어 잡화', '캠핑 낚시 캠핑', '캠핑 낚시 낚시', '구기 라켓 구기 스포츠', '구기 라켓 라켓 스포츠', '구기 라켓 이외 스포츠 용품', '골프 골프 의류 잡화', '골프 골프클럽', '골프 골프 용품', '자전거 자전거', '자전거 자전거 의류 잡화', '자전거 자전거 용품', '자전거 자전거 부품', '스케이트 보드 휠 레저', '스케이트 보드 킥 보드 스케이트 보드', '스키 스노우보드 스키 보드 용품', '스키 스노우보드 스키 스노우보드 복', '스키 스노우보드 스노우보드 장비', '스키 스노우보드 스키 장비', '격투기 복싱 격투기', '격투기 복싱 검도', '격투기 복싱 태권도 유도', '격투기 복싱 마라톤 육상 용품', '격투기 복싱 이외 격투스포츠', '스포츠 액세서리 안전 용품 보호 장비', '스포츠 액세서리 안전 용품 스포츠 액세서리', '지역 서비스 맛집 뷔페 카페', '지역 서비스 키즈 여가 문화', '지역 서비스 뷰티 운동', '지역 서비스 생활 편의', '쿠폰 선물 모바일 상품권', '쿠폰 선물 지류 상품권', '쿠폰 선물 디지털 컨텐츠 러닝', '쿠폰 선물 치킨 피자 패스트푸드', '쿠폰 선물 카페 베이커리 분식', '쿠폰 선물 외식 뷔페 프랜차이즈', '쿠폰 선물 아이스크림 빙수', '쿠폰 선물 문화 영화 생활', '쿠폰 선물 편의점 마트 면세점', '뮤지컬 리지 공연', '뮤지컬 라이선스', '뮤지컬 창작 뮤지컬', '뮤지컬 넌버벌', '콘서트 국내 공연', '콘서트 내한 공연', '콘서트 페스티벌', '콘서트', '연극 대학로 대학로', '연극 대학로 연극', '클래식 무용 클래식', '클래식 무용 오페라', '클래식 무용 무용', '클래식 무용 국악', '전시 체험 전시회', '전시 체험 체험 전', '전시 체험 행사 박람회', '키즈 가족 뮤지컬 연극', '키즈 가족 체험 전', '키즈 가족 클래식 무용'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_tag_sim_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vir_py3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
